"""
æ¨¡å‹è°ƒåº¦å™¨

è´Ÿè´£æ ¹æ®é…ç½®å†³å®šä½¿ç”¨ CLI æˆ– MCP è°ƒç”¨ Codex/Geminiã€‚
v5.4.0: CLI ä¼˜å…ˆæ¨¡å¼ï¼ŒçœŸå®è°ƒç”¨å¤–éƒ¨æ¨¡å‹ã€‚
"""

import os
import subprocess
import shlex
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Optional, List
import json
import time

from .models import SkillpackConfig
from .usage import UsageStore, UsageRecord


class ModelType(Enum):
    """æ¨¡å‹ç±»å‹"""
    CLAUDE = "claude"
    CODEX = "codex"
    GEMINI = "gemini"


class ExecutionMode(Enum):
    """æ‰§è¡Œæ¨¡å¼"""
    CLI = "cli"
    MCP = "mcp"


@dataclass
class DispatchResult:
    """è°ƒåº¦ç»“æœ"""
    success: bool
    output: str
    error: Optional[str] = None
    model: Optional[ModelType] = None
    mode: Optional[ExecutionMode] = None
    duration_ms: int = 0
    command: Optional[str] = None  # å®é™…æ‰§è¡Œçš„å‘½ä»¤


class ModelDispatcher:
    """
    æ¨¡å‹è°ƒåº¦å™¨ - æ ¹æ®é…ç½®å†³å®šä½¿ç”¨ CLI æˆ– MCP è°ƒç”¨æ¨¡å‹ã€‚

    v5.3+: CLI ä¼˜å…ˆæ¨¡å¼ï¼Œç¦æ­¢ MCP è°ƒç”¨ã€‚
    """

    def __init__(self, config: SkillpackConfig):
        self.config = config
        self.use_cli = config.cli.prefer_cli_over_mcp
        self._execution_log: List[dict] = []
        self._mock_mode = self._detect_mock_mode()
        # ç”¨é‡è¿½è¸ª
        self._usage_store = UsageStore()
        self._current_task_id: Optional[str] = None
        self._current_route: Optional[str] = None
        self._current_phase: int = 0
        self._current_phase_name: str = ""

    def set_context(
        self,
        task_id: str,
        route: str,
        phase: int = 0,
        phase_name: str = ""
    ):
        """è®¾ç½®å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆåœ¨æ‰§è¡Œå™¨ä¸­è°ƒç”¨ï¼‰"""
        self._current_task_id = task_id
        self._current_route = route
        self._current_phase = phase
        self._current_phase_name = phase_name

    def _detect_mock_mode(self) -> bool:
        """æ£€æµ‹æ˜¯å¦å¯ç”¨ mock æ¨¡å¼ï¼ˆæµ‹è¯•ç¯å¢ƒé¿å…çœŸå®è°ƒç”¨å¤–éƒ¨ CLIï¼‰"""
        return bool(os.environ.get("SKILLPACK_MOCK_MODE") or os.environ.get("PYTEST_CURRENT_TEST"))

    def _mock_result(self, model: ModelType, prompt: str) -> DispatchResult:
        """ç”Ÿæˆ mock è°ƒç”¨ç»“æœ"""
        preview = (prompt or "").strip().replace("\n", " ")[:200]
        output = f"[mock {model.value} output] {preview}"
        return DispatchResult(
            success=True,
            output=output,
            model=model,
            mode=ExecutionMode.CLI,
            duration_ms=0,
            command="mock"
        )

    def get_execution_mode(self) -> ExecutionMode:
        """è·å–å½“å‰æ‰§è¡Œæ¨¡å¼"""
        return ExecutionMode.CLI if self.use_cli else ExecutionMode.MCP

    def call_codex(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None,
        sandbox: str = "workspace-write"
    ) -> DispatchResult:
        """
        è°ƒç”¨ Codex æ¨¡å‹ã€‚

        Args:
            prompt: ä»»åŠ¡æç¤º
            context_files: ç›¸å…³æ–‡ä»¶åˆ—è¡¨
            sandbox: æ²™ç®±æ¨¡å¼ (read-only, workspace-write, danger-full-access)

        Returns:
            DispatchResult åŒ…å«æ‰§è¡Œç»“æœ
        """
        if self._mock_mode:
            return self._mock_result(ModelType.CODEX, prompt)
        if self.use_cli:
            return self._call_codex_cli(prompt, context_files, sandbox)
        else:
            return self._call_codex_mcp(prompt, context_files, sandbox)

    def call_gemini(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None,
        sandbox: bool = True
    ) -> DispatchResult:
        """
        è°ƒç”¨ Gemini æ¨¡å‹ã€‚

        Args:
            prompt: ä»»åŠ¡æç¤º
            context_files: ç›¸å…³æ–‡ä»¶åˆ—è¡¨ï¼ˆä½¿ç”¨ @ è¯­æ³•æ³¨å…¥ï¼‰
            sandbox: æ˜¯å¦ä½¿ç”¨æ²™ç®±æ¨¡å¼

        Returns:
            DispatchResult åŒ…å«æ‰§è¡Œç»“æœ
        """
        if self._mock_mode:
            return self._mock_result(ModelType.GEMINI, prompt)
        if self.use_cli:
            return self._call_gemini_cli(prompt, context_files, sandbox)
        else:
            return self._call_gemini_mcp(prompt, context_files, sandbox)

    def query_knowledge_base(
        self,
        notebook_id: str,
        query: str
    ) -> Optional[str]:
        """
        æŸ¥è¯¢ NotebookLM çŸ¥è¯†åº“ã€‚

        Args:
            notebook_id: NotebookLM ç¬”è®°æœ¬ ID
            query: æŸ¥è¯¢å†…å®¹

        Returns:
            æŸ¥è¯¢ç»“æœæ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
        """
        # å…ˆæ£€æŸ¥ notebook_idï¼ˆæ— è®ºæ˜¯å¦ mock æ¨¡å¼ï¼‰
        if not notebook_id:
            return None

        if self._mock_mode:
            return f"[mock knowledge base response] Query: {query[:100]}"

        start_time = time.time()

        try:
            # ä½¿ç”¨ MCP å·¥å…·æŸ¥è¯¢ NotebookLM
            # æ³¨æ„ï¼šè¿™é‡Œè¿”å› MCP è°ƒç”¨å‚æ•°ï¼Œç”±è°ƒç”¨æ–¹å®é™…æ‰§è¡Œ
            # å› ä¸º MCP è°ƒç”¨éœ€è¦åœ¨ Claude ä¸Šä¸‹æ–‡ä¸­å®Œæˆ
            return {
                "tool": "mcp__notebooklm-mcp__notebook_query",
                "params": {
                    "notebook_id": notebook_id,
                    "query": query
                }
            }
        except Exception as e:
            return None

    def format_knowledge_query_prompt(
        self,
        task_description: str,
        phase_name: str
    ) -> str:
        """
        ç”ŸæˆçŸ¥è¯†åº“æŸ¥è¯¢ promptã€‚

        Args:
            task_description: ä»»åŠ¡æè¿°
            phase_name: å½“å‰é˜¶æ®µåç§°

        Returns:
            æ ¼å¼åŒ–çš„æŸ¥è¯¢ prompt
        """
        return f"""æŸ¥è¯¢ä¸ä»¥ä¸‹ä»»åŠ¡ç›¸å…³çš„éœ€æ±‚æ–‡æ¡£å’ŒéªŒæ”¶æ ‡å‡†ï¼š

ä»»åŠ¡æè¿°: {task_description}
å½“å‰é˜¶æ®µ: {phase_name}

è¯·è¿”å›ï¼š
1. ç›¸å…³çš„åŠŸèƒ½éœ€æ±‚
2. éªŒæ”¶æ ‡å‡†å’Œæµ‹è¯•ç”¨ä¾‹
3. æŠ€æœ¯çº¦æŸå’Œæ³¨æ„äº‹é¡¹
4. ç›¸å…³çš„è®¾è®¡å†³ç­–"""

    def _call_codex_cli(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None,
        sandbox: str = "workspace-write"
    ) -> DispatchResult:
        """
        é€šè¿‡ CLI è°ƒç”¨ Codexã€‚

        å‘½ä»¤æ ¼å¼: codex exec "<prompt>" --full-auto
        """
        start_time = time.time()

        # æ„å»ºå®Œæ•´ promptï¼ˆåŒ…å«æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼‰
        full_prompt = self._build_prompt_with_context(prompt, context_files)

        # æ„å»ºå‘½ä»¤
        # --full-auto = -a on-request + -s workspace-write
        cmd = [
            self.config.cli.codex_command,
            "exec",
            full_prompt,
            "--full-auto"
        ]

        # å¦‚æœ sandbox ä¸æ˜¯é»˜è®¤å€¼ï¼Œæ˜¾å¼æŒ‡å®š
        if sandbox != "workspace-write":
            cmd = [
                self.config.cli.codex_command,
                "exec",
                full_prompt,
                "-s", sandbox,
                "-a", "on-request"
            ]

        command_str = f"{self.config.cli.codex_command} exec \"<prompt>\" --full-auto"

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self.config.cli.cli_timeout_seconds,
                cwd=Path.cwd()
            )

            duration_ms = int((time.time() - start_time) * 1000)

            self._log_execution(
                model=ModelType.CODEX,
                mode=ExecutionMode.CLI,
                success=result.returncode == 0,
                duration_ms=duration_ms,
                command=command_str
            )

            if result.returncode == 0:
                return DispatchResult(
                    success=True,
                    output=result.stdout,
                    model=ModelType.CODEX,
                    mode=ExecutionMode.CLI,
                    duration_ms=duration_ms,
                    command=command_str
                )
            else:
                return DispatchResult(
                    success=False,
                    output=result.stdout,
                    error=result.stderr or f"Exit code: {result.returncode}",
                    model=ModelType.CODEX,
                    mode=ExecutionMode.CLI,
                    duration_ms=duration_ms,
                    command=command_str
                )

        except subprocess.TimeoutExpired:
            duration_ms = int((time.time() - start_time) * 1000)
            return DispatchResult(
                success=False,
                output="",
                error=f"Codex CLI è¶…æ—¶ ({self.config.cli.cli_timeout_seconds}s)",
                model=ModelType.CODEX,
                mode=ExecutionMode.CLI,
                duration_ms=duration_ms,
                command=command_str
            )
        except FileNotFoundError:
            return DispatchResult(
                success=False,
                output="",
                error=f"Codex CLI æœªæ‰¾åˆ°: {self.config.cli.codex_command}",
                model=ModelType.CODEX,
                mode=ExecutionMode.CLI,
                command=command_str
            )
        except Exception as e:
            return DispatchResult(
                success=False,
                output="",
                error=f"Codex CLI æ‰§è¡Œå¤±è´¥: {str(e)}",
                model=ModelType.CODEX,
                mode=ExecutionMode.CLI,
                command=command_str
            )

    def _call_gemini_cli(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None,
        sandbox: bool = True
    ) -> DispatchResult:
        """
        é€šè¿‡ CLI è°ƒç”¨ Geminiã€‚

        å‘½ä»¤æ ¼å¼: gemini "<prompt>" -s --yolo
        """
        start_time = time.time()

        # Gemini ä½¿ç”¨ @ è¯­æ³•æ³¨å…¥æ–‡ä»¶ä¸Šä¸‹æ–‡
        full_prompt = self._build_gemini_prompt(prompt, context_files)

        # æ„å»ºå‘½ä»¤
        cmd = [self.config.cli.gemini_command, full_prompt]

        if sandbox:
            cmd.append("-s")

        # --yolo è‡ªåŠ¨æ‰¹å‡†æ‰€æœ‰æ“ä½œ
        cmd.append("--yolo")

        command_str = f"{self.config.cli.gemini_command} \"<prompt>\" -s --yolo"

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self.config.cli.cli_timeout_seconds,
                cwd=Path.cwd()
            )

            duration_ms = int((time.time() - start_time) * 1000)

            self._log_execution(
                model=ModelType.GEMINI,
                mode=ExecutionMode.CLI,
                success=result.returncode == 0,
                duration_ms=duration_ms,
                command=command_str
            )

            if result.returncode == 0:
                return DispatchResult(
                    success=True,
                    output=result.stdout,
                    model=ModelType.GEMINI,
                    mode=ExecutionMode.CLI,
                    duration_ms=duration_ms,
                    command=command_str
                )
            else:
                return DispatchResult(
                    success=False,
                    output=result.stdout,
                    error=result.stderr or f"Exit code: {result.returncode}",
                    model=ModelType.GEMINI,
                    mode=ExecutionMode.CLI,
                    duration_ms=duration_ms,
                    command=command_str
                )

        except subprocess.TimeoutExpired:
            duration_ms = int((time.time() - start_time) * 1000)
            return DispatchResult(
                success=False,
                output="",
                error=f"Gemini CLI è¶…æ—¶ ({self.config.cli.cli_timeout_seconds}s)",
                model=ModelType.GEMINI,
                mode=ExecutionMode.CLI,
                duration_ms=duration_ms,
                command=command_str
            )
        except FileNotFoundError:
            return DispatchResult(
                success=False,
                output="",
                error=f"Gemini CLI æœªæ‰¾åˆ°: {self.config.cli.gemini_command}",
                model=ModelType.GEMINI,
                mode=ExecutionMode.CLI,
                command=command_str
            )
        except Exception as e:
            return DispatchResult(
                success=False,
                output="",
                error=f"Gemini CLI æ‰§è¡Œå¤±è´¥: {str(e)}",
                model=ModelType.GEMINI,
                mode=ExecutionMode.CLI,
                command=command_str
            )

    def _call_codex_mcp(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None,
        sandbox: str = "workspace-write"
    ) -> DispatchResult:
        """
        é€šè¿‡ MCP è°ƒç”¨ Codexã€‚

        æ³¨æ„ï¼šå½“ cli.prefer_cli_over_mcp=true æ—¶ï¼Œæ­¤æ–¹æ³•ä¸ä¼šè¢«è°ƒç”¨ã€‚
        æ­¤æ–¹æ³•æä¾›ç»™ MCP æ¨¡å¼ä½¿ç”¨ï¼Œè¿”å›è°ƒç”¨å‚æ•°ä¾› Claude ä½¿ç”¨ MCP å·¥å…·ã€‚
        """
        full_prompt = self._build_prompt_with_context(prompt, context_files)

        # MCP æ¨¡å¼ï¼šè¿”å›è°ƒç”¨å‚æ•°ï¼Œç”± Claude æ‰§è¡Œ MCP è°ƒç”¨
        mcp_params = {
            "tool": "mcp__codex-cli__codex",
            "params": {
                "prompt": full_prompt,
                "sandbox": sandbox
            }
        }

        return DispatchResult(
            success=True,
            output=json.dumps(mcp_params, ensure_ascii=False, indent=2),
            model=ModelType.CODEX,
            mode=ExecutionMode.MCP,
            command="mcp__codex-cli__codex"
        )

    def _call_gemini_mcp(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None,
        sandbox: bool = True
    ) -> DispatchResult:
        """
        é€šè¿‡ MCP è°ƒç”¨ Geminiã€‚

        æ³¨æ„ï¼šå½“ cli.prefer_cli_over_mcp=true æ—¶ï¼Œæ­¤æ–¹æ³•ä¸ä¼šè¢«è°ƒç”¨ã€‚
        """
        full_prompt = self._build_gemini_prompt(prompt, context_files)

        mcp_params = {
            "tool": "mcp__gemini-cli__ask-gemini",
            "params": {
                "prompt": full_prompt,
                "sandbox": sandbox
            }
        }

        return DispatchResult(
            success=True,
            output=json.dumps(mcp_params, ensure_ascii=False, indent=2),
            model=ModelType.GEMINI,
            mode=ExecutionMode.MCP,
            command="mcp__gemini-cli__ask-gemini"
        )

    def _build_prompt_with_context(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None
    ) -> str:
        """æ„å»ºåŒ…å«æ–‡ä»¶ä¸Šä¸‹æ–‡çš„ prompt"""
        if not context_files or not self.config.cli.auto_context:
            return prompt

        # è¯»å–æ–‡ä»¶å†…å®¹
        context_parts = []
        for file_path in context_files[:self.config.cli.max_context_files]:
            try:
                path = Path(file_path)
                if path.exists() and path.is_file():
                    lines = path.read_text().splitlines()
                    # é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°
                    if len(lines) > self.config.cli.max_lines_per_file:
                        lines = lines[:self.config.cli.max_lines_per_file]
                        lines.append(f"... (truncated at {self.config.cli.max_lines_per_file} lines)")
                    content = "\n".join(lines)
                    context_parts.append(f"### {file_path}\n```\n{content}\n```")
            except Exception:
                continue

        if context_parts:
            context_section = "\n\n".join(context_parts)
            return f"{prompt}\n\nç›¸å…³æ–‡ä»¶:\n{context_section}"

        return prompt

    def _build_gemini_prompt(
        self,
        prompt: str,
        context_files: Optional[List[str]] = None
    ) -> str:
        """æ„å»º Gemini promptï¼ˆä½¿ç”¨ @ è¯­æ³•ï¼‰"""
        if not context_files:
            return prompt

        # Gemini ä½¿ç”¨ @ è¯­æ³•å¼•ç”¨æ–‡ä»¶
        file_refs = " ".join(f"@{f}" for f in context_files[:self.config.cli.max_context_files])
        return f"{file_refs} {prompt}"

    def _log_execution(
        self,
        model: ModelType,
        mode: ExecutionMode,
        success: bool,
        duration_ms: int,
        command: str,
        error: Optional[str] = None
    ):
        """è®°å½•æ‰§è¡Œæ—¥å¿—ï¼ˆå†…å­˜ + æŒä¹…åŒ–ï¼‰"""
        timestamp = datetime.now().isoformat()

        # å†…å­˜æ—¥å¿—
        self._execution_log.append({
            "timestamp": timestamp,
            "model": model.value,
            "mode": mode.value,
            "success": success,
            "duration_ms": duration_ms,
            "command": command
        })

        # æŒä¹…åŒ–è®°å½•
        record = UsageRecord(
            timestamp=timestamp,
            model=model.value,
            route=self._current_route or "UNKNOWN",
            phase=self._current_phase,
            phase_name=self._current_phase_name,
            task_id=self._current_task_id,
            success=success,
            duration_ms=duration_ms,
            error=error,
            mode=mode.value
        )
        self._usage_store.append_record(record)

    def get_execution_log(self) -> List[dict]:
        """è·å–æ‰§è¡Œæ—¥å¿—"""
        return self._execution_log.copy()

    def format_phase_header(
        self,
        phase: int,
        total_phases: int,
        phase_name: str,
        route: str,
        model: ModelType,
        progress_percent: int
    ) -> str:
        """
        æ ¼å¼åŒ–é˜¶æ®µå¤´éƒ¨è¾“å‡ºã€‚

        æ ¹æ®æ‰§è¡Œæ¨¡å¼æ˜¾ç¤ºä¸åŒçš„æ ‡è¯†ã€‚
        """
        mode_str = "CLI" if self.use_cli else "MCP å¼ºåˆ¶è°ƒç”¨"
        model_name = model.value.capitalize()

        if model == ModelType.CLAUDE:
            mode_str = "ç›´æ¥æ‰§è¡Œ"

        # æ„å»ºè¿›åº¦æ¡
        progress_bar = self._build_progress_bar(progress_percent)

        return f"""â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ Phase {phase}/{total_phases}: {phase_name} | {route} è·¯ç”±
ğŸ¤– æ‰§è¡Œæ¨¡å‹: {model_name} ({mode_str})
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è¿›åº¦: {progress_bar} {progress_percent}%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"""

    def _build_progress_bar(self, percent: int, width: int = 20) -> str:
        """æ„å»ºè¿›åº¦æ¡"""
        filled = int(width * percent / 100)
        empty = width - filled
        return f"[{'â–ˆ' * filled}{'â–‘' * empty}]"

    def format_phase_complete(
        self,
        phase: int,
        model: ModelType,
        duration_ms: int,
        output_file: str,
        degraded: bool = False,
        original_model: Optional[ModelType] = None
    ) -> str:
        """æ ¼å¼åŒ–é˜¶æ®µå®Œæˆè¾“å‡º"""
        mode_str = "CLI" if self.use_cli else "MCP"
        model_name = model.value.capitalize()
        duration_str = f"{duration_ms / 1000:.1f}s"

        if degraded and original_model:
            return f"""âš ï¸ Phase {phase} å®Œæˆ (é™çº§æ‰§è¡Œ)
â”œâ”€â”€ åŸè®¡åˆ’æ¨¡å‹: {original_model.value.capitalize()}
â”œâ”€â”€ å®é™…æ¨¡å‹: {model_name} (ç”¨æˆ·æˆæƒé™çº§)
â”œâ”€â”€ é™çº§åŸå› : MCP è°ƒç”¨å¤±è´¥
â””â”€â”€ è¾“å‡º: {output_file}"""

        return f"""âœ… Phase {phase} å®Œæˆ
â”œâ”€â”€ æ‰§è¡Œæ¨¡å‹: {model_name}
â”œâ”€â”€ æ‰§è¡Œæ¨¡å¼: {mode_str}
â”œâ”€â”€ è€—æ—¶: {duration_str}
â””â”€â”€ è¾“å‡º: {output_file}"""


# ä¾¿æ·å‡½æ•°ï¼šè·å–è°ƒåº¦å™¨å®ä¾‹
def get_dispatcher(config: SkillpackConfig) -> ModelDispatcher:
    """è·å–æ¨¡å‹è°ƒåº¦å™¨å®ä¾‹"""
    return ModelDispatcher(config)
